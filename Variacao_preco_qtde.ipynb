{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "## Notebook para cria\u00e7\u00e3o da base com as quantidades vendidas pelos valores m\u00ednimos, m\u00e1ximos e m\u00e9dia por:\n- SKU;\n- Estado (Regional);\n\n#### A ideia \u00e9 dar uma vis\u00e3o para o lojista dos pre\u00e7os praticados nacionalmente e por regi\u00e3o (estado)"}, {"metadata": {}, "cell_type": "markdown", "source": "##### Iniciando o Notebook Incluir no projeto o token para exposi\u00e7\u00e3o na nuvem caso seja necess\u00e1rio"}, {"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(spark.sparkContext, 'de02faf2-fbe4-466f-b8cd-44296f096a9d', 'p-25957a2b4ca812c11c01435cf691ea3068358702')\npc = project.project_context", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20210530185816-0000\nKERNEL_ID = dc93b153-965d-4cd9-a3be-237d55209b8f\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "**Importando algumas bibliotecas Utilizadas no projeto**"}, {"metadata": {}, "cell_type": "code", "source": "import sys\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Obter os dados e criar as tabelas tempor\u00e1rias para utiliza\u00e7\u00e3o com comandos SQL no spark**"}, {"metadata": {}, "cell_type": "code", "source": "dfcompraentregastatus = spark.read.parquet(project.get_file_url(\"compraentregastatus.parquet\"))", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dfcompraentrega = spark.read.parquet(project.get_file_url(\"compraentrega.snappy.parquet\"))", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Criando uma tabela temporaria\ndfcompraentrega.createOrReplaceTempView(\"cprEntrega\")\ndfcompraentregastatus.createOrReplaceTempView(\"cprEntregaStaus\")", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dfvariacao = spark.read.parquet(project.get_file_url(\"variacao_preco.parquet\"))\ndfvariacao.createOrReplaceTempView(\"variacao\")", "execution_count": 6, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dfcprSku = spark.read.parquet(project.get_file_url(\"compraentregasku.snappy.parquet\"))\ndfcprSku.createOrReplaceTempView(\"compraSKU\")", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dfcompra = spark.read.parquet(project.get_file_url(\"compra.snappy.parquet\"))\ndfcompra.createOrReplaceTempView(\"compra\")", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Jun\u00e7\u00e3o das tabelas de compra juntamente com a tabela de varia\u00e7\u00e3o de pre\u00e7o criado no outro notebook**\npara obter os valores m\u00e1ximos, m\u00ednimo pratidos e o intervalo entre esses valores (valor entre)"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"SELECT tmp.IDSKUREFERENCIA, sum(qtde_min) as qtde_min, \\\n           sum(qtde_max) as qtde_max, \\\n           case when sum(total - qtde_min - qtde_max) < 0 then 0 \\\n            else sum(total - qtde_min - qtde_max) end as qtde_entre from (\\\n           select V.IDSKUREFERENCIA, 0 as qtde_min, 0 as qtde_max, count(*) as total \\\n             from compra     C \\\n                 ,cprEntrega E \\\n                 ,compraSKU  S \\\n                 ,variacao   V \\\n           where C.IDCOMPRA = E.IDCOMPRA \\\n             and E.IDCOMPRAENTREGA = S.IDCOMPRAENTREGA \\\n             and S.IDSKUREFERENCIA = V.IDSKUREFERENCIA \\\n             and cast(C.data as date) >= cast('2021-04-30' as date) - 60 \\\n             and E.idlojista in (10012,37450,92893,80333,64189)\\\n             group by V.IDSKUREFERENCIA\\\n          union all \\\n          select V.IDSKUREFERENCIA, count(*) as qtde_min, 0 as qtde_max, 0 as total \\\n            from compra     C \\\n                ,cprEntrega E \\\n                ,compraSKU  S \\\n                ,variacao   V \\\n           where C.IDCOMPRA = E.IDCOMPRA \\\n             and E.IDCOMPRAENTREGA = S.IDCOMPRAENTREGA \\\n             and S.IDSKUREFERENCIA = V.IDSKUREFERENCIA \\\n             and cast(C.data as date) >= cast('2021-04-30' as date) - 60 \\\n             and VALORVENDAUNIDADESEMDESCONTO = V.minVrVenda  \\\n             and E.idlojista in (10012,37450,92893,80333,64189)\\\n             group by V.IDSKUREFERENCIA\\\n             union all \\\n          select V.IDSKUREFERENCIA, 0 as qtde_min, count(*) as qtde_max, 0 as total \\\n            from compra     C \\\n                ,cprEntrega E \\\n                ,compraSKU  S \\\n                ,variacao   V \\\n           where C.IDCOMPRA = E.IDCOMPRA \\\n             and E.IDCOMPRAENTREGA = S.IDCOMPRAENTREGA \\\n             and S.IDSKUREFERENCIA = V.IDSKUREFERENCIA \\\n             and cast(C.data as date) >= cast('2021-04-30' as date) - 60 \\\n             and VALORVENDAUNIDADESEMDESCONTO = V.maxVrVenda  \\\n             and E.idlojista in (10012,37450,92893,80333,64189)\\\n             group by V.IDSKUREFERENCIA\\\n             ) as tmp \\\n             group by tmp.IDSKUREFERENCIA \\\n             \").show(truncate=False)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "+---------------+--------+--------+----------+\n|IDSKUREFERENCIA|qtde_min|qtde_max|qtde_entre|\n+---------------+--------+--------+----------+\n|8154506        |2       |2       |0         |\n|1504540530     |1       |1       |0         |\n|1504559122     |1       |1       |0         |\n|1509144134     |1       |1       |0         |\n|1504183454     |1       |1       |0         |\n|1504540521     |1       |1       |0         |\n|1501283149     |2       |2       |0         |\n|1502998468     |1       |1       |0         |\n|1502998701     |2       |2       |0         |\n|1509146521     |1       |1       |0         |\n|1512120618     |5       |5       |0         |\n|1500125888     |1       |1       |0         |\n|1504775619     |1       |1       |0         |\n|1503675043     |1       |1       |0         |\n|1503042197     |1       |1       |0         |\n|13553255       |2       |1       |9         |\n|1502998707     |2       |2       |0         |\n|1501281810     |1       |1       |0         |\n|1510076731     |1       |1       |0         |\n|1502998643     |1       |1       |0         |\n+---------------+--------+--------+----------+\nonly showing top 20 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "**Tabela tempor\u00e1ria** Cria a tabela gerada em dataframe para grava\u00e7\u00e3o em disco"}, {"metadata": {}, "cell_type": "code", "source": "dfTabelaFinal = spark.sql(\"SELECT tmp.IDSKUREFERENCIA, sum(qtde_min) as qtde_min, \\\n           sum(qtde_max) as qtde_max, \\\n           case when sum(total - qtde_min - qtde_max) < 0 then 0 \\\n            else sum(total - qtde_min - qtde_max) end as qtde_entre from (\\\n           select V.IDSKUREFERENCIA, 0 as qtde_min, 0 as qtde_max, count(*) as total \\\n             from compra     C \\\n                 ,cprEntrega E \\\n                 ,compraSKU  S \\\n                 ,variacao   V \\\n           where C.IDCOMPRA = E.IDCOMPRA \\\n             and E.IDCOMPRAENTREGA = S.IDCOMPRAENTREGA \\\n             and S.IDSKUREFERENCIA = V.IDSKUREFERENCIA \\\n             and cast(C.data as date) >= cast('2021-04-30' as date) - 60 \\\n             and E.idlojista in (10012,37450,92893,80333,64189)\\\n             group by V.IDSKUREFERENCIA\\\n          union all \\\n          select V.IDSKUREFERENCIA, count(*) as qtde_min, 0 as qtde_max, 0 as total \\\n            from compra     C \\\n                ,cprEntrega E \\\n                ,compraSKU  S \\\n                ,variacao   V \\\n           where C.IDCOMPRA = E.IDCOMPRA \\\n             and E.IDCOMPRAENTREGA = S.IDCOMPRAENTREGA \\\n             and S.IDSKUREFERENCIA = V.IDSKUREFERENCIA \\\n             and cast(C.data as date) >= cast('2021-04-30' as date) - 60 \\\n             and VALORVENDAUNIDADESEMDESCONTO = V.minVrVenda  \\\n             and E.idlojista in (10012,37450,92893,80333,64189)\\\n             group by V.IDSKUREFERENCIA\\\n             union all \\\n          select V.IDSKUREFERENCIA, 0 as qtde_min, count(*) as qtde_max, 0 as total \\\n            from compra     C \\\n                ,cprEntrega E \\\n                ,compraSKU  S \\\n                ,variacao   V \\\n           where C.IDCOMPRA = E.IDCOMPRA \\\n             and E.IDCOMPRAENTREGA = S.IDCOMPRAENTREGA \\\n             and S.IDSKUREFERENCIA = V.IDSKUREFERENCIA \\\n             and cast(C.data as date) >= cast('2021-04-30' as date) - 60 \\\n             and VALORVENDAUNIDADESEMDESCONTO = V.maxVrVenda  \\\n             and E.idlojista in (10012,37450,92893,80333,64189)\\\n             group by V.IDSKUREFERENCIA\\\n             ) as tmp \\\n             group by tmp.IDSKUREFERENCIA \\\n             \")", "execution_count": 10, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**GRAVA\u00c7\u00c3O EM DISCO** Os pr\u00f3ximos passos \u00e9 para grava\u00e7\u00e3o da tabela gerada em disco e disponibilizar nos ativos"}, {"metadata": {}, "cell_type": "code", "source": "dfTabelaFinal.repartition(1).write.mode(\"overwrite\").parquet(\"/home/spark/shared/tmp/compra_produto.parquet\")", "execution_count": 14, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dfTabelaFinal.repartition(1).write.mode(\"overwrite\").csv('/home/spark/shared/tmp/variacao_preco_qtde.csv')", "execution_count": 12, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Obtendo credenciais para disponibilizar nos ativos\ncredentials_1 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-f8ddc357-a261-42a2-a6f2-44ac38d6cc50',\n    'IBM_API_KEY_ID': '5y97tXSIMk-o11IIlG-qhORjxg4zQ6yFprxqB54ntyX9',\n    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'projetoviagrupo6-donotdelete-pr-ehmgbn0j3d7dfc',\n}\n\ncgsClient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id = credentials_1['IBM_API_KEY_ID'],\n    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')", "execution_count": 50, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ls tmp/variacao_preco_qtde.parquet/", "execution_count": 51, "outputs": [{"name": "stdout", "output_type": "stream", "text": "part-00000-f8ebc172-c6f1-4300-b5b8-52447749d310-c000.snappy.parquet  _SUCCESS\r\n"}]}, {"metadata": {}, "cell_type": "code", "source": "cgsClient.upload_file(Filename='tmp/variacao_preco_qtde.parquet/part-00000-f8ebc172-c6f1-4300-b5b8-52447749d310-c000.snappy.parquet',Bucket=credentials_1['BUCKET'],Key='variacao_preco_qtde.parquet')", "execution_count": 57, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "ls tmp/variacao_preco_qtde.csv/", "execution_count": 54, "outputs": [{"name": "stdout", "output_type": "stream", "text": "part-00000-b9bf1c06-36b0-4d05-8dff-0cbf27aa2c22-c000.csv  _SUCCESS\r\n"}]}, {"metadata": {}, "cell_type": "code", "source": "cgsClient.upload_file(Filename='tmp/variacao_preco_qtde.csv/part-00000-b9bf1c06-36b0-4d05-8dff-0cbf27aa2c22-c000.csv',Bucket=credentials_1['BUCKET'],Key='variacao_preco_qtde.csv')", "execution_count": 56, "outputs": []}], "metadata": {"kernelspec": {"name": "python37", "display_name": "Python 3.7 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.7.10", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}